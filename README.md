# Implementing a Transformer From Scratch with PyTorch  

A practical deep learning project focused on building a **Transformer** model from scratch using **PyTorch**, without relying on high-level libraries like `torch.nn.Transformer`. The project is designed to deepen theoretical understanding through hands-on implementation of attention mechanisms, positional encoding, and encoder-decoder architecture.

## ğŸ”§ Technologies Used

- **Language:** Python 3.12.0 
- **Frameworks:** PyTorch, NumPy  
- **Tools:** VSCode, Git  
- **Key Concepts:**  
  - Attention Mechanism  
  - Multi-Head Attention  
  - Layer Normalization  
  - Positional Encoding  
  - Masking Techniques  
  - Encoder-Decoder Architecture  

### ğŸ“ Project Structure

```plaintext
transformer-from-scratch/
|
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ batch.py               # Holds a batch training data
â”‚   â”œâ”€â”€ synthetic_data.py      # Generation of synthetic training data
|
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ attention.py           # Scaled dot-product & multi-head attention
â”‚   â”œâ”€â”€ encoder.py             # Transformer encoder block
â”‚   â”œâ”€â”€ decoder.py             # Transformer decoder block
â”‚   â”œâ”€â”€ generator.py           # Linear layer + softmax output probabilities generation step
â”‚   â”œâ”€â”€ transformer.py         # Full model integration
â”‚   â”œâ”€â”€ feed_forward.py        # Position-wise feed-forward network
â”‚   â”œâ”€â”€ embeddings.py          # Token and positional embeddings
|   â””â”€â”€ utils.py               # Helper functions, masking, cloning etc 
|
â”œâ”€â”€ train/                     # (Planned) Model training
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ loss.py                # (Planned) Loss computation
â”‚   â”œâ”€â”€ rate.py                # Implementation of the NoamOptimizer, adjusting the learning rate.
â”‚   â”œâ”€â”€ state.py               # TrainState dataclass
â”‚   â”œâ”€â”€ train.py               # Training loop
|
â”œâ”€â”€ main.py                    # Sample forward pass & debugging
â”œâ”€â”€ inference.py               # (Planned) Inference script
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Project documentation
```

## Licensing

The source code in this project is released under the MIT License.  
You are free to use, modify, and distribute this software under the terms of the license.

See the full license text in the [LICENSE](LICENSE) file.


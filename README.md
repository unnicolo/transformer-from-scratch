# Implementing a Transformer From Scratch with PyTorch  

A practical deep learning project focused on building a **Transformer** model from scratch using **PyTorch**, without relying on high-level libraries like `torch.nn.Transformer`. The project is designed to deepen theoretical understanding through hands-on implementation of attention mechanisms, positional encoding, and encoder-decoder architecture.

## ğŸ”§ Technologies Used

- **Language:** Python 3.12.0 
- **Frameworks:** PyTorch, NumPy  
- **Tools:** VSCode, Git  
- **Key Concepts:**  
  - Attention Mechanism  
  - Multi-Head Attention  
  - Layer Normalization  
  - Positional Encoding  
  - Masking Techniques  
  - Encoder-Decoder Architecture  

### ğŸ“ Project Structure

```plaintext
transformer-from-scratch/
|
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ batch.py               # Holds a batch training data
â”‚   â”œâ”€â”€ tokenizer.py           # Handles loading SpaCy tokenizers and basic text
â”‚   â”œâ”€â”€ synthetic_data.py      # Generation of synthetic training data
|
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ attention.py           # Scaled dot-product & multi-head attention
â”‚   â”œâ”€â”€ encoder.py             # Transformer encoder block
â”‚   â”œâ”€â”€ decoder.py             # Transformer decoder block
â”‚   â”œâ”€â”€ generator.py           # Linear layer + softmax output probabilities generation step
â”‚   â”œâ”€â”€ transformer.py         # Full model integration
â”‚   â”œâ”€â”€ feed_forward.py        # Position-wise feed-forward network
|   â””â”€â”€ embddings.py           # Token and positional embeddings
|
â”œâ”€â”€ train/                     # Model training
â”‚   â”œâ”€â”€ __init__.py           
â”‚   â”œâ”€â”€ decode.py              # Various decoding methods - just greedy decoding, fow now
â”‚   â”œâ”€â”€ label_smoothing.py     # Label smoothing to avoid overfitting and overconfidence
â”‚   â”œâ”€â”€ loss.py                # Loss computation
â”‚   â”œâ”€â”€ rate.py                # Implementation of the NoamOptimizer, adjusting the learning rate
â”‚   â”œâ”€â”€ state.py               # TrainState dataclass
|   â””â”€â”€ train.py               # Training loop
|
â”œâ”€â”€ main.py                    # Sample forward pass & debugging
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ utils.py                   # Helper classes and functions
â””â”€â”€ README.md                  # Project documentation
```

## Licensing

The source code in this project is released under the MIT License.  
You are free to use, modify, and distribute this software under the terms of the license.

See the full license text in the [LICENSE](LICENSE) file.

